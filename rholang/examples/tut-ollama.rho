new chatResponse, generateResponse, modelsResponse,
    ollamaChat(`rho:ollama:chat`),
    ollamaGenerate(`rho:ollama:generate`),
    ollamaModels(`rho:ollama:models`),
    stdout(`rho:io:stdout`) in {

  // Test chat completion with specific model
  ollamaChat!("llama4:latest", "What is blockchain technology in one sentence?", *chatResponse) |
  for(@answer <- chatResponse) {
    stdout!(["Ollama Chat Response:", answer])
  } |

  // Test chat completion with default model (no model specified)
  ollamaChat!("Explain smart contracts briefly", *chatResponse) |
  for(@defaultAnswer <- chatResponse) {
    stdout!(["Ollama Default Model Response:", defaultAnswer])
  } |

  // Test text generation
  ollamaGenerate!("llama4:latest", "Write a haiku about decentralized computing:\n", *generateResponse) |
  for(@poem <- generateResponse) {
    stdout!(["Ollama Generated Haiku:", poem])
  } |

  // Test text generation with default model
  ollamaGenerate!("Complete this: The future of blockchain is", *generateResponse) |
  for(@completion <- generateResponse) {
    stdout!(["Ollama Text Completion:", completion])
  } |

  // List available models
  ollamaModels!(*modelsResponse) |
  for(@models <- modelsResponse) {
    stdout!(["Available Ollama Models:", models])
  }
}